{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark Concepts\n",
    "\n",
    "### History\n",
    "\n",
    "- Motivation\n",
    "    - Move computing to data, not data to computing\n",
    "- Google\n",
    "    Google Distributed Filesystem (GFS)\n",
    "    Big Table\n",
    "    Map-reduce\n",
    "- Yahoo!\n",
    "    - Hadoop Distributed File System (HDFS)\n",
    "    - Yet Anohter Resource Negotiator (YARN)\n",
    "    - MapReduce\n",
    "- Limitations of MapReduce\n",
    "    - Cumbersome API\n",
    "    - Every stage reads from/writes to disk\n",
    "    - No native interactive SQL (HIVE, Impala, Drill)\n",
    "    - No native streaming (Storm)\n",
    "    - No native mahcine learning (Mahout)\n",
    "- Spark\n",
    "    - Simple API\n",
    "    - In-memory storage\n",
    "    - Better fault tolerance\n",
    "    - Can take advantage of embarrassingly parallel computations\n",
    "    - Multi-language support (Scala, Java, Python, R)\n",
    "    - Support multiple workloads\n",
    "    - Spark 1.0 released May 11, 2014\n",
    "- Speed\n",
    "    - DAG scheduler\n",
    "    - Catalyst query optimizer\n",
    "    - Tungsten execution engine\n",
    "- Ease of use\n",
    "    - Resilient Distributed Dataset (RDD)\n",
    "    - Lazy evaluation\n",
    "- Modularity\n",
    "    - Languages\n",
    "    - APIs - Strucrtured Srtreaming, GraphX, MLLib\n",
    "- Extensible\n",
    "    - Data readers\n",
    "    - Data writers\n",
    "    - [3rd party connectors](https://spark.apache.org/third-party-projects.html)\n",
    "    - Multiple platforms - local, data center, cloud, kubernetes\n",
    "- Distributed execution concepts\n",
    "    - Spark driver\n",
    "        - Spark session\n",
    "        - Spark shell\n",
    "        - Communicates with Spark Master\n",
    "        - Communicates with Spakr workers\n",
    "    - Spark master\n",
    "        - Resource management on cluster\n",
    "    - Spark workers\n",
    "        - Communicate reosuces to cluster manger\n",
    "        - Start Spark Executors\n",
    "    - Spark executors\n",
    "       - Communicate with driver\n",
    "       - Runs task\n",
    "       - Can run multiple threds in parallel\n",
    "- Execution process\n",
    "    - Driver creates jobs\n",
    "        - Each job is a DAG\n",
    "        - DAGScheduler translates into physical plan using RDDs\n",
    "        - Optimization incldues merging and splitiing into stages\n",
    "        - TaskScheduler distributes physical plans to Executors\n",
    "    - Job consists of one or more stages\n",
    "        - Stage normally ends when there is a need to exchange data (shuffle)\n",
    "    - Stage consists of tasks\n",
    "        - A task is a unit of execution\n",
    "        - Each task is sent to one executor and assigned one data partition\n",
    "        - A mutli-core computer can run several tasks in parallel\n",
    "- Transforms and actions\n",
    "    - Transforms are lazy\n",
    "    - Actions are eager\n",
    "    - NArrow versus broad transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install Spark\n",
    "\n",
    "- If necessary, install [Java](https://java.com/en/download/help/download_options.xml)\n",
    "- Downlaod and install [Sppark](http://spark.apache.org/downloads.html)\n",
    "```bash\n",
    "wget http://apache.mirrors.pair.com/spark/spark-2.4.4/spark-2.4.4-bin-hadoop2.7.tgz\n",
    "tar xzf spark-2.4.4-bin-hadoop2.7.tgz\n",
    "sudo mv spark-2.4.4-bin-hadoop2.7 /usr/local/spark\n",
    "```\n",
    "Set up graphframes\n",
    "```bash\n",
    "pip install graphframes\n",
    "```\n",
    "Set up environment variables\n",
    "```bash\n",
    "export PATH=$PATH:/usr/local/spark/bin\n",
    "export SPARK_HOME=/usr/local/spark\n",
    "export PYTHONPATH=$SPARK_HOME/python:$PYTHONPATH\n",
    "export PYSPARK_DRIVER_PYTHON=\"jupyter\"\n",
    "export PYSPARK_DRIVER_PYTHON_OPTS=\"notebook\"\n",
    "export PYSPARK_PYTHON=python3\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark UI\n",
    "\n",
    "- Default port 4040 http://localhost:4040/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%file candy.csv\n",
    "name,age,candy\n",
    "tom,3,m&m\n",
    "shirley,6,mentos\n",
    "david,4,candy floss\n",
    "anne,5,starburst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.csv('candy.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show(n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
